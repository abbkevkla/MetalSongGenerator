{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S8kevkla\\Downloads\\TrainMusic\\MetalSongGenerator\\musicautobot-master\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\S8kevkla\\Downloads\\TrainMusic\\MetalSongGenerator\\musicautobot-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musicautobot.numpy_encode import *\n",
    "from musicautobot.utils.file_processing import process_all, process_file\n",
    "from musicautobot.config import *\n",
    "from musicautobot.music_transformer import *\n",
    "from musicautobot.multitask_transformer import *\n",
    "from musicautobot.utils.stacked_dataloader import StackedDataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultitaskTransformer Training\n",
    "\n",
    "Multitask Training is an extension of [MusicTransformer](../music_transformer/Train.ipynb).\n",
    "\n",
    "Instead a basic language model that predicts the next word...\n",
    "\n",
    "We train on multiple tasks\n",
    "* [Next Word](../music_transformer/Train.ipynb)\n",
    "* [Bert Mask](https://arxiv.org/abs/1810.04805)\n",
    "* [Sequence to Sequence Translation](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "This gives a more generalized model and also let's you do some really cool [predictions](Generate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end training pipeline \n",
    "\n",
    "1. Create and encode dataset\n",
    "2. Initialize Transformer MOdel\n",
    "3. Train\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of your midi files\n",
    "midi_path = Path('data/midi/Meshuggah_midi')\n",
    "midi_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Location to save dataset\n",
    "data_path = Path('data/numpy')\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_save_name = 'musicitem_data_save.pkl'\n",
    "s2s_data_save_name = 'multiitem_data_save.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather midi dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all your midi data is in `musicautobot/data/midi` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a pretty good dataset with lots of midi data:  \n",
    "https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/\n",
    "\n",
    "Download the folder and unzip it to `data/midi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create dataset from MIDI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_files = get_files(midi_path, '.mid', recurse=True); len(midi_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Create NextWord/Mask Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S8kevkla\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\fastai\\data_block.py:457: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  or pass `ignore_empty=True` when labelling to remove this warning.\"\"\")\n"
     ]
    }
   ],
   "source": [
    "processors = [Midi2ItemProcessor()]\n",
    "data = MusicDataBunch.from_files(midi_files, data_path, processors=processors, \n",
    "                                 encode_position=True, dl_tfms=mask_lm_tfm_pitchdur, \n",
    "                                 bptt=5, bs=2, ignore_empty=True)\n",
    "data.save(data_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msk': {'x': tensor([[50,  4, 43,  4, 43],\n",
       "          [50,  4, 43,  4, 43]]), 'pos': tensor([[32, 32, 32, 32, 32],\n",
       "          [32, 32, 32, 32, 32]])},\n",
       " 'lm': {'x': tensor([[138,  50, 138,  43, 138],\n",
       "          [138,  50, 138,  43, 138]]), 'pos': tensor([[32, 32, 32, 32, 32],\n",
       "          [32, 32, 32, 32, 32]])}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = data.one_batch(); xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key:\n",
    "* 'msk' = masked input\n",
    "* 'lm' = next word input\n",
    "* 'pos' = timestepped postional encoding. This is in addition to relative positional encoding\n",
    "\n",
    "Note: MultitaskTransformer trains on both the masked input ('msk') and next word input ('lm') at the same time.\n",
    "\n",
    "The encoder is trained on the 'msk' data, while the decoder is trained on 'lm' data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Create sequence to sequence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S8kevkla\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\fastai\\data_block.py:457: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  or pass `ignore_empty=True` when labelling to remove this warning.\"\"\")\n"
     ]
    }
   ],
   "source": [
    "processors = [Midi2MultitrackProcessor()]\n",
    "s2s_data = MusicDataBunch.from_files(midi_files, data_path, processors=processors, \n",
    "                                     preloader_cls=S2SPreloader, list_cls=S2SItemList,\n",
    "                                     dl_tfms=melody_chord_tfm,\n",
    "                                     bptt=5, bs=2)\n",
    "s2s_data.save(s2s_data_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c2m': {'enc': tensor([[  5,   1,   8, 169,  53],\n",
       "          [  5,   1,   8, 153,  50]], dtype=torch.int32),\n",
       "  'enc_pos': tensor([[ 0,  0,  0,  0, 32],\n",
       "          [ 0,  0,  0,  0, 16]], dtype=torch.int32),\n",
       "  'dec': tensor([[  6,   1,   8, 169,  53],\n",
       "          [  6,   1,   8, 153,  70]], dtype=torch.int32),\n",
       "  'dec_pos': tensor([[ 0,  0,  0,  0, 32],\n",
       "          [ 0,  0,  0,  0, 16]], dtype=torch.int32)},\n",
       " 'm2c': {'enc': tensor([[  6,   1,   8, 169,  53],\n",
       "          [  6,   1,   8, 153,  70]], dtype=torch.int32),\n",
       "  'enc_pos': tensor([[ 0,  0,  0,  0, 32],\n",
       "          [ 0,  0,  0,  0, 16]], dtype=torch.int32),\n",
       "  'dec': tensor([[  5,   1,   8, 169,  53],\n",
       "          [  5,   1,   8, 153,  50]], dtype=torch.int32),\n",
       "  'dec_pos': tensor([[ 0,  0,  0,  0, 32],\n",
       "          [ 0,  0,  0,  0, 16]], dtype=torch.int32)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = s2s_data.one_batch(); xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key:\n",
    "* 'c2m' = chord2melody translation\n",
    " * enc = chord\n",
    " * dec = melody\n",
    "* 'm2c' = next word input\n",
    " * enc = melody\n",
    " * dec = chord\n",
    "* 'pos' = timestepped postional encoding. Gives the model a better reference when translating\n",
    "\n",
    "Note: MultitaskTransformer trains both translations ('m2c' and 'c2m') at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "batch_size = 2\n",
    "bptt = 128\n",
    "\n",
    "lm_data = load_data(data_path, data_save_name, \n",
    "                    bs=batch_size, bptt=bptt, encode_position=True,\n",
    "                    dl_tfms=mask_lm_tfm_pitchdur)\n",
    "\n",
    "s2s_data = load_data(data_path, s2s_data_save_name, \n",
    "                     bs=batch_size//2, bptt=bptt,\n",
    "                     preloader_cls=S2SPreloader, dl_tfms=melody_chord_tfm)\n",
    "\n",
    "# Combine both dataloaders so we can train multiple tasks at the same time\n",
    "data = StackedDataBunch([lm_data, s2s_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "config = multitask_config(); config\n",
    "\n",
    "learn = multitask_model_learner(data, config.copy())\n",
    "# learn.to_fp16(dynamic=True) # Enable for mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTransformer(\n",
       "  (encoder): MTEncoder(\n",
       "    (embed): TransformerEmbedding(\n",
       "      (embed): Embedding(312, 512, padding_idx=1)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (beat_enc): Embedding(32, 512, padding_idx=0)\n",
       "      (bar_enc): Embedding(1024, 512, padding_idx=0)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): MTEncoder(\n",
       "    (embed): TransformerEmbedding(\n",
       "      (embed): Embedding(312, 512, padding_idx=1)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (beat_enc): Embedding(32, 512, padding_idx=0)\n",
       "      (bar_enc): Embedding(1024, 512, padding_idx=0)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): MTEncoderBlock(\n",
       "        (mha1): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mha2): MemMultiHeadRelativeAttentionKV(\n",
       "          (q_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_wgt): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): MTLinearDecoder(\n",
       "    (decoder): Linear(in_features=512, out_features=312, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mask_acc</th>\n",
       "      <th>lm_acc</th>\n",
       "      <th>c2m_acc</th>\n",
       "      <th>m2c_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='100' class='' max='111', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.09% [100/111 04:33<00:30 5.8380]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4dfb24161c57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\fastai\\train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[1;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[0;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[0;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m->\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\TrainMusic\\MetalSongGenerator\\musicautobot-master\\musicautobot\\multitask_transformer\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mc2m\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mc2m_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc2m\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'enc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2m\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'enc_pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mc2m_dec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc2m\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dec'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2m\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dec_pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2m_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c2m'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc2m_dec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\TrainMusic\\MetalSongGenerator\\musicautobot-master\\musicautobot\\multitask_transformer\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_lm, lm_pos, msk_emb)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlm_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_lm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mlm_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_lm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmsk_emb\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmsk_emb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlm_emb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mpos_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelative_pos_enc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsk_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\TrainMusic\\MetalSongGenerator\\musicautobot-master\\musicautobot\\multitask_transformer\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inp, pos)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbeat_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeat_enc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeat_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mbar_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeat_len\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_bar_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mbar_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbar_pos\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_bar_len\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_bar_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/numpy/models/meshuggah_model.pth')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('meshuggah_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "---\n",
    "See [Generate.ipynb](Generate.ipynb) to use a pretrained model and generate better predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi_files = get_files(midi_path, '.mid', recurse=True)\n",
    "midi_file = Path('data/midi/notebook_examples/single_bar_example.mid'); midi_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = nw_predict_from_midi(learn, midi_file, n_words=20, seed_len=8); next_word.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_melody = s2s_predict_from_midi(learn, midi_file, n_words=20, seed_len=4, pred_melody=True); pred_melody.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_notes = mask_predict_from_midi(learn, midi_file, predict_notes=True); pred_notes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
